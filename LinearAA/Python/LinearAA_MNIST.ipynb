{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9dc40477",
   "metadata": {},
   "source": [
    "## Runing the Linear AA on the MNIST dataset\n",
    "- For the Benoulli method it requires adaptation, since the original MNIST isn't binarized, and this implementation was created for binary data.\n",
    "- Binarizing your MNIST data is a straightforward process involving setting a threshold.Standard MNIST Data: A pixel $x_{i,j} \\in \\{0, 1, 2, \\dots, 255\\}$.Binarization Step: You apply a threshold, $T$, to every pixel in the dataset. A common practice for MNIST is to set $T=127.5$ (or 128) since the values range up to 255.The resulting binary pixel $x'_{i,j}$ would be calculated as:$$x'_{i,j} = \\begin{cases} 1 & \\text{if } x_{i,j} > T \\\\ 0 & \\text{if } x_{i,j} \\le T \\end{cases}$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "12175717",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torchvision import datasets, transforms\n",
    "from src.methods.AABernoulli import Bernoulli_Archetypal_Analysis\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt \n",
    "from src.methods.AALS import AALS "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a9256d02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading MNIST...\n",
      "torch.Size([5842, 784])\n",
      "Data Ready: torch.Size([784, 5842]) (Features x Samples)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/cz/_btxbwrj7cx6d5v2vzg25g740000gn/T/ipykernel_90184/2764844498.py:15: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_raw = torch.tensor(data_digit).float()\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 1. Load and Preprocess MNIST (The \"Binary\" way)\n",
    "print(\"Loading MNIST...\")\n",
    "# Download MNIST\n",
    "mnist_data = datasets.MNIST(root='./data', train=True, download=True, transform=transforms.ToTensor())\n",
    "\n",
    "# pick single digit\n",
    "digit = 4\n",
    "idx_digit = mnist_data.targets.numpy() == digit\n",
    "data_digit = mnist_data.data[idx_digit,]\n",
    "data_digit = np.reshape(data_digit, (data_digit.shape[0], -1))\n",
    "print(data_digit.shape)\n",
    "\n",
    "# Use all samples\n",
    "n_samples = len(data_digit)\n",
    "X_raw = torch.tensor(data_digit).float()\n",
    "X = X_raw.flatten(start_dim=1).T \n",
    "\n",
    "X = X.double() \n",
    "\n",
    "print(f\"Data Ready: {X.shape} (Features x Samples)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce59385d",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "062a9130",
   "metadata": {},
   "source": [
    "### Guassian Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6168d0b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run 1/10\n",
      "Run 2/10\n",
      "Run 3/10\n",
      "Run 4/10\n",
      "Run 5/10\n",
      "Run 6/10\n",
      "Run 7/10\n",
      "Run 8/10\n",
      "Run 9/10\n",
      "Run 10/10\n"
     ]
    }
   ],
   "source": [
    "n_runs = 10\n",
    "n_archetypes = 3\n",
    "\n",
    "S_list = []\n",
    "C_list = []\n",
    "L_list = []  # <-- store the losses\n",
    "\n",
    "for run in range(n_runs):\n",
    "    print(f\"Run {run+1}/{n_runs}\")\n",
    "    C_run, S_run, L_run, _ = AALS(X, n_archetypes)  # Your Linear AA function\n",
    "    \n",
    "    # Append to lists\n",
    "    S_list.append(S_run.detach().cpu().numpy() if isinstance(S_run, torch.Tensor) else S_run)\n",
    "    C_list.append(C_run.detach().cpu().numpy() if isinstance(C_run, torch.Tensor) else C_run)\n",
    "    L_list.append(L_run)  # <-- add this line\n",
    "\n",
    "# Save everything\n",
    "torch.save({'C': C_list, 'S': S_list, 'L': L_list}, 'mnist_gaussian_aa_results.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "85ae5520",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Archetype Matrix shape for plotting: (784, 3)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABC8AAAERCAYAAABb+Y1HAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjcsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvTLEjVAAAAAlwSFlzAAAPYQAAD2EBqD+naQAAHYtJREFUeJzt3XuIXFfhB/A7O7PvzWazSTZpsk3aJDWtqaYGWyvaUqX+Y1Wq1hci+MAHSBEE+4ciIigIivjWP8QX9QFqxRdYtGqLTYtN27S0Nc370Wzem+wr+575cQcqVvs7Z9ubzZ7d/Xwg2PU7e++Zuztz5n73zpxSrVarZQAAAACJapjrAQAAAACEKC8AAACApCkvAAAAgKQpLwAAAICkKS8AAACApCkvAAAAgKQpLwAAAICkKS8AAACApCkvAAAAgKQpLwAAAICkKS/mme985ztZqVTKXvWqV13U/X73u9/N3vGOd2Tr1q2r7//973//Rd0/AAt3njly5Ej2+c9/PrvuuuuyZcuWZStWrMhuuumm7C9/+ctFGwMAC3uuGR0dzT70oQ9lV199dbZ06dKso6Mj27p1a/b1r389m5ycvGjj4MUr1Wq1WoHv5yJ7zWtek/X19WUHDx7M9uzZk23atOmi7Peyyy7LhoaG6i8s8xeT733ve7Mf/ehHF2XfACzseeZb3/pWdscdd2S33nprff9TU1PZT37yk+yRRx7JfvCDH2Qf+MAHZn0MACzsuaa/vz974xvfmN144431c5uGhoZs+/bt2Z133pm9+93vzn72s5/N+hgoRnkxjxw4cCDbsGFDdtddd2Uf/ehHs49//OPZ5z73uej35S8Cq9Vq1tTU9KL3fejQoX9fdZG3lLfddpvyAmCBmat55sknn8xWrVpVv+LiWePj49k111yTDQ8P16/MAGBhmMtzmudz++2310v0Y8eOZatXr76g2+bC8raReeSnP/1p/XLaW265pV4e5F//t7y9zAuGr3zlK9nXvva1bOPGjVlzc3P21FNP1fNdu3Zl73znO7OVK1dmra2t2ebNm7PPfOYz0X2vX7++vl0AFq65mme2bNnynOIil28z/wvZM888U7/yD4CFYS7PaZ5PfhVG7ty5cwXvGbOtMut74ILJH9hve9vb6m3je97znvrnUDz00EPZtdde+z+3/eEPf5iNjY1lH/nIR+oP9O7u7uzxxx/PbrjhhqyxsbH+/+cP1H379mW///3vsy9+8Yt+UgCLXGrzzPHjx7O2trb6PwAWhrmeayYmJrLBwcH6Z2Ds2LGjXpDkf6i9WG/H58VTXswTDz/8cL1h/OY3v1n/+rWvfW3W29tbf/A/3wM9/0vV3r17623ks973vvdl+Uec5O8hzt8C8qwvfelLF+leAJCq1OaZfNv5JcX5h0WXy+UXfb8ASEcKc00+t+SlybNe+cpX1j9fqVJxapw6bxuZJ/IHdP5+4Ne97nX1r/PLqN71rndlv/jFL7Lp6en/uf3b3/725zzIT506ld13333ZBz/4wec8yJ/dFgCLW0rzzPnz5+ulRX4psIIdYOFIYa7J9/3nP/85++Uvf5l97GMfq1/BMTIyUvi+MfuUF/NA/kDOH9D5Ay3/gJu8fcz/5UsLnThxIrvnnnv+53suv/zy53y9f//++v/mSwMBQKrzTD6W/FPf8/c1/+pXv8rWrFnjhwWwAKQy1+Tlyc0331z/vI38LStvetObsje84Q31tyqSNuXFPPDXv/61/um3+YP9iiuu+Pe//ENqcs/3ITf5X6sAYL7NMx/+8IezP/zhD/UVrV7/+tfPyj4AWNxzzX/KS4x8Zavf/va3s74vivHGnnkgfyD39PRk3/72t5/3PVu/+c1vsu9973vBB3e+HFHuiSeemNWxAjD/pDLPfOpTn6p/OFv+yfL/+X5kAOa/VOaa/5Z/cGduYGDggm2T2aG8SFz+YHr2A8vyVvC/5ZfT/vznP89+97vf1d8v9v/J3yt244031j+M5pOf/ORz3iOWf+CNz70AWJxSmWe+/OUv1z/x/dOf/nT2iU98ouC9AiAlKcw1p0+fzpYvX/4/t/n+97//7w/uJG3Ki8TlD+B8ffu3vOUtz5tff/319Qdx3mSGHui5b3zjG/VP9N22bVt9WaH8PWT5Gsp//OMfs507dwa/N1966LHHHqv/9+TkZH2Joi984Qv1r/OxvfzlL3/R9xGAxT3P5H9tu+OOO+qXD1911VXZnXfe+Zw8fy9y/h5lAOanFOaafG7Jr+y49dZb61dw5OO5++676x/e+eY3v9lbFecB5UXi8gdwS0tL/YXb82loaMhuueWW+u3OnDkT3NbWrVuzBx98MPvsZz9b/3CafM3kfE3jZ99nFvLrX/86+/GPf/zvrx999NH6v1y+vJHyAmB+SmGeebYc37NnT30JvP/2t7/9TXkBMI+lMNfkhcf27dvrV3jkHxCaL426efPm7Ktf/Wp2++23F7p/XBylWn59DQAAAECirDYCAAAAJE15AQAAACRNeQEAAAAkTXkBAAAAJE15AQAAACRNeQEAAAAkTXkBAAAAJE15AQAAACRNeQEAAAAkTXkBAAAAJE15AQAAACRNeQEAAAAkTXkBAAAAJE15AQAAACRNeQEAAAAkTXkBAAAAJE15AQAAACRNeQEAAAAkTXkBAAAAJE15AQAAACRNeQEAAAAkTXkBAAAAJE15AQAAACStMtcDAAAAgIWoVqsV3kapVLogY5nvXHkBAAAAJE15AQAAACRNeQEAAAAkTXkBAAAAJE15AQAAACRNeQEAAAAkTXkBAAAAJE15AQAAACStMtcDgBerWq0G81KpVCgHAAC8Fp/r85qYcrmcLQauvAAAAACSprwAAAAAkqa8AAAAAJKmvAAAAACSprwAAAAAkqa8AAAAAJKmvAAAAACSVpnrAfDCTU1NBfNarRbMS6VS4bWCZ7KNokZHRwvlk5OThddDbmxsDOatra3BvKmpKboPgIVmenr6gtymyFw4Pj4e3UZDQ/hvOB0dHYXnEYC5EDsfiL1OnslzXOx5OPZafSbPoUuWLMnm2sTERDAfGRkpvI/YOUfsWDVGvj+V87uiXHkBAAAAJE15AQAAACRNeQEAAAAkTXkBAAAAJE15AQAAACRNeQEAAAAkTXkBAAAAJK0y1wPgf1Wr1eBhOX/+fDAfGBgofFi7u7uDeWtrazBvaAj3YtPT09ExnD17Npg/9dRTwXxoaCiYt7S0RMewevXqYN7b2xvMly1bFswrFQ9B4IWZmpoK5uPj49FtjI2NFXoOj60FH/v+mcxlBw8eDOanT58uPM9s3LgxmG/evDmYl8vl6D6AxadWqxV6foqdC+QmJyeD+fDwcDDv7+8vPN/E7mfsfixZsiQ6hvb29sLzTdH7GTsniZ17xea7XHNzc6Fzr2WRc45cW1tbMG9sbMxS58oLAAAAIGnKCwAAACBpygsAAAAgacoLAAAAIGnKCwAAACBpygsAAAAgacoLAAAAIGmVuR7AYhNbDzk3NDQUzA8cOBDMT506VWgd4ZmsmRxba7joesq5PXv2BPMnnnii0JrMK1asyIqKrU8dyysVD0FYbM/x4+PjwbxarQbz0dHRYH706NHoGGLb6OzsDOZLly4tPEfE5qonn3wymO/evbvwevXt7e3BfPPmzdFtAPy3UqlUKI89R8/kNseOHQvmJ0+ejO4jdpumpqZg3tXVFczL5fIFmVeLmp6eDuYjIyPB/PDhw8H8+PHj0THE5t3Yect05D7MZO6O/bxi54ex/EJw5QUAAACQNOUFAAAAkDTlBQAAAJA05QUAAACQNOUFAAAAkDTlBQAAAJA05QUAAACQNOUFAAAAkLTKXA9gsZmamorepq+vL5j/4x//CObnzp0L5hs2bIiOYc2aNcG8VCplRZw+fTp6m6effjqY7969O5hXKuFf7+XLl0fH0NAQ7veam5uDea1Wi+4DWFxiz59jY2PBfN++fcH86NGj0THE9rFp06Zg3t3dHcwnJiaiYzhz5kyh+/HII48E87Vr10bHMD09XWgOAHgxYq8PZ3K+MDg4OKvPobmhoaFCc8GWLVuCeWtra3QMRc85ZmJycrJQHpvPDh48GB1DR0dHMG9sbAzm7e3thfcR+71MYU6c+xEAAAAABCgvAAAAgKQpLwAAAICkKS8AAACApCkvAAAAgKQpLwAAAICkKS8AAACApFXmegCLTX9/f/Q2f//734P5vffeG8xbWlqC+SWXXBIdw0zWXQ6Znp4O5nv27Ilu4+mnnw7mzzzzTDDftGlTMO/s7IyOIXasYuslNzU1RfcBLBwzWY++UglPvSdPngzmhw8fDub79++PjqGrq6vQc1e5XA7mExMT0TGMjo4G82PHjgXzvXv3BvNVq1ZFx9DT01PofgLMhsnJyehtzpw5U+i1dux1dG5wcLDQc2Rsvlu2bFl0DA0Ns/+39lqtVmhOi81XM5mXV6xYEczXrl1beN6N/TwuxrEuKv0RAgAAAIua8gIAAABImvICAAAASJryAgAAAEia8gIAAABImvICAAAASJryAgAAAEhaeLFXXrDx8fFg/tBDD0W38ac//SmY7969O5i/+tWvDuarVq2KjqG9vT2Yl0qlYN7f3x/MH3/88egYYutTx3R3dwfz9evXR7exfPnyYN7a2lroOAGLT2w9+YGBgWC+b9++YH7w4MHoGK666qpgPj09XSivVqvRMYyNjQXzvr6+YD4yMhLM29raomPo6emJ3gbgQou9PpyYmIhu4/Tp08H8wIEDwfzQoUPRfUxOTgbzq6++utDr6K6uriwF5XI5mA8PDwfzw4cPFz7WsXk1do5ZjtyHXKVSmffnLa68AAAAAJKmvAAAAACSprwAAAAAkqa8AAAAAJKmvAAAAACSprwAAAAAkqa8AAAAAJIWXuyVF7x2/f79+4P53XffHT2qjz76aDDv7OwM5ldccUUw37hxY3QMjY2Nhdaffuyxx4L5jh07omM4ceJEMN+6dWsw37RpUzBft25ddAyxYx07TvNhvWTg4hoZGQnmJ0+eDOZ79+4ttFb8TJ6bYnNdbB8NDQ2Fj8Phw4eD+djYWDBftWpV4ed4gNkwNTUVzAcGBqLbOHLkSKH8wIED0X2sX7++0DnHlVdeGczL5XKWgtg4hoaGgvnRo0eDeV9fX3QMPT09hX5nGiPnJLmmpqZsvnPlBQAAAJA05QUAAACQNOUFAAAAkDTlBQAAAJA05QUAAACQNOUFAAAAkDTlBQAAAJA05QUAAACQtMpcD2C+OXPmTDC/++67g/l9990X3cfIyEgwv+mmm4L59ddfH8x7enqiY6jVasH86NGjwfz+++8P5k8//XR0DO3t7cF8w4YNwXzjxo3BvKOjIzqGpqamYN7QEO7/SqVSdB/AwlGtVqO3GR8fD+anTp0K5gMDA8G8tbW18PNruVwO5pVKpdB9zA0ODgbz48ePB/OJiYlgvnz58ugYWlpaorcBuNBiz19DQ0PRbRw7diyY79q1q/A+Vq5cGcw3bdoUzJcsWZLNB7HX68PDw8H8xIkTwby/vz86hqmpqULzbntkXp/J3B47r0lB+iMEAAAAFjXlBQAAAJA05QUAAACQNOUFAAAAkDTlBQAAAJA05QUAAACQNOUFAAAAkLTwgrGL0Llz54L5vffeG8zvueeeYH748OHoGGJrJt94443B/LLLLgvm1Wo1OoazZ88G8/vvvz+Yb9++PZgPDg5Gx7Bly5Zg/tKXvjSYX3LJJYXXQ46t+xw7lrH1lIHFZ3R0NJiPjIwUmqdmYnp6OpiPj48XmiPOnz8fHUNfX18wP336dDDv6OiY9TXvAWZDrVYL5hMTE9Ft7N27t9Bz7NKlS6P72LBhQzBfs2ZNodfRF2K+uhDfH5t3T5w4EcyPHDlSaN7PVSrh0/IlS5YE87a2tsK/d7HzmoaGub/uYe5HAAAAABCgvAAAAACSprwAAAAAkqa8AAAAAJKmvAAAAACSprwAAAAAkqa8AAAAAJIWXlB2gRkeHo7eZufOncH8gQceCOa7du0qvO78K17ximDe09MTzM+dOxfMz549Gx3D7t27g/ldd91V6DisXr06OoZNmzYF8+7u7kJrFcfymYitlwzwQp83BgYGgvmxY8cKzQG5Z555Jph3dHQE876+vmB+/Pjx6BiefPLJYD41NRXMW1tbC69HH9tHU1NTdBsAF9rJkyejtzly5EgwHxsbC+ZtbW3RfcRer5fL5ULz0Uxei09PTxe6n7E8NzQ0FMz/9a9/BfP+/v5gPj4+Hh1D7FhOTEwUvp+xbcRen7S0tGRzzZUXAAAAQNKUFwAAAEDSlBcAAABA0pQXAAAAQNKUFwAAAEDSlBcAAABA0pQXAAAAQNIq2QISW0N39+7d0W3s2LEjmO/cubPQmvG9vb3RMcTWXT548GAwP3HiRKG1iHPbt28P5v/85z8LrRO8fPny6Biam5uD+fnz54P58PBwVlRXV1fhNbIB/lNDQ0OhfHJyMpifO3cuesAffPDBYL5nz55Ca72Pjo5Gx3Do0KGsiEqlUmiOyJ09e7bQHNDU1JTNtlKpNOv7AC6u2GvUmbyGjd0mNlfMxIEDB4L5Aw88EMxbW1sLzxWxc6vY+d/09HR0H8eOHSt0fjg4OFh4rogdi6LHYSa/E7F5NQWuvAAAAACSprwAAAAAkqa8AAAAAJKmvAAAAACSprwAAAAAkqa8AAAAAJKmvAAAAACSprwAAAAAklbJFpD+/v5gfvjw4eg2jhw5EszPnz8fzFtbW4N5S0tLdAyxcZ4+fTqYj4+PB/Pjx49Hx7Br165gPjo6GszXrVsXzFevXh0dQ3t7ezCv1WrBfHh4uPDPYmJiotDPG1hcGhrifxNoa2sL5itWrAjmW7ZsKTzXnT17NphXq9VgvmTJkmA+PT0dHUPsOTymsbGx0H3IDQ0NBfPOzs5stpVKpVnfB3BxTU1NFXodPTk5Gd1H7DVoc3NzofOF3I4dO4L5sWPHgnlTU1Oh5/GZHIvYfDOT+Sh2fhebV2cy38TEthF7fTE9g/sZO6+J/Txi+yiXy9lsc+UFAAAAkDTlBQAAAJA05QUAAACQNOUFAAAAkDTlBQAAAJA05QUAAACQNOUFAAAAkLRKNo/E1r8dGxsrvI/Ymsm9vb3BfNWqVYXWXJ7JmsixNXqPHz8ezJ944onoGM6dOxfML7nkkmC+devWYL5t27boGDZu3BjMe3p6gnlHR0cwb29vj46hpaUlmJdKpeg2AP5TV1dX8IDccMMNhdZhj80BM5kvY89tsblscHAwOoaTJ08Wmgu7u7uD+bp166JjWLp0aaExxJgjYHGq1WqFnsdXrlwZ3ceVV14ZzIeGhoL5mTNnovvYv39/MD948GCh19ozOS+KbSP2PB2bc2cyJ46MjES3UXQMsfOazs7OYN7W1hbdR7lcLnQsGxrm/rqHuR8BAAAAQIDyAgAAAEia8gIAAABImvICAAAASJryAgAAAEia8gIAAABImvICAAAASFolm0dia8u2tLQE8xUrVkT3sXXr1mC+fv36YD49PR3MJycno2OoVquF1mU+cuRIMD937lx0DLFjedNNNwXz2267LZhfffXV0TEsW7as0NrQsTW0Y3muVColv94xML9UKuGp9/LLLw/mq1evDuaDg4PRMYyPjxeah2LPfbt3746OYefOncG8tbU1mK9bty6Yb9y4MTqGrq6uYF4ul6PbAHihzx0dHR3BfO3atdGDet111wXz9vb2YN7X1xfdx9TUVDCv1WqFjkNbW1t0DLHzt9ixjD3P5/bt2xfMH3/88WA+PDxcaN7OrVmzptD9bI6cF83keMden6TAmRcAAACQNOUFAAAAkDTlBQAAAJA05QUAAACQNOUFAAAAkDTlBQAAAJA05QUAAACQtPQXc30BOjs7g/mmTZsKr7E7NjYWzCcmJoL55ORkdAwDAwPB/OGHHw7mIyMjWVEve9nLgvlb3/rWYH7zzTcX+lkBLFYtLS2F1nJfunRpdB+1Wi2Yl0qlQt/f19cXHcP4+HgwL5fLwby3tzeYd3V1RcfQ2NgYvQ3AC9XQEP77cHt7ezDv6emJ7uPaa68N5pdeemkwHx4eju5jamoqmDc1NQXztra2Qs/zM5kTY3nsPszkWMTOW2Lnh7GfRW7t2rXBfPXq1YWO9Ux+L2Nirw0uBldeAAAAAElTXgAAAABJU14AAAAASVNeAAAAAElTXgAAAABJU14AAAAASVNeAAAAAElTXgAAAABJq2QLSHNzczCvVOJ3t6urK5hPTU0F8/Hx8WA+PDwcHcPAwEAwP3jwYDDv7+8P5itXroyO4aabbgrmN9xwQzBfsmRJdB8AvHClUqlQfiFMT08XmodyExMTwbypqSmYL1u2LJg3NjZGxwCQ4jlLT09PdBvt7e3BvLe3N5hXq9XoPhoaGgrlsefxWq0WHUO5XC405506dSq6j9i5USyfnJwM5t3d3dExdHZ2FjrWra2ts34sU+DKCwAAACBpygsAAAAgacoLAAAAIGnKCwAAACBpygsAAAAgacoLAAAAIGnKCwAAACBplWwBqVQqhda2nYnYGrvj4+PBfHR0NLqPQ4cOFconJiaC+TXXXBMdw+te97pC6x3Ph3WCAXhxYnPdyZMno9uIzYdLly4N5l1dXcG8Wq1GxwCQotbW1uhtGhsbC+1jJq/Vi547xc6barVa4XHGtnHq1KnCx7K7uzuYj42NFf55xs5jp6amgvnk5GThn8d8OH9z5QUAAACQNOUFAAAAkDTlBQAAAJA05QUAAACQNOUFAAAAkDTlBQAAAJA05QUAAACQtPCCsgvMTNauja0VHFtDd2hoKJgfPnw4Oobdu3cH89OnTxdai3jbtm3RMVx11VWzuu4zAPNXbE37U6dORbcRW7N+5cqVwby5uTmYV6vV6BgA5us5S6VSmfV9zOQ2sz2GonPJ8PBw4X20t7cH87Vr1xaar2Zyjhk79xofH4/uIzYvtra2BvOGhrm/7mHuRwAAAAAQoLwAAAAAkqa8AAAAAJKmvAAAAACSprwAAAAAkqa8AAAAAJKmvAAAAACSVmyB4AVoYmKiUD4wMBDMjx8/Hh3DqVOnCq2xu3HjxmC+bdu26BhWrFgRvQ0Ai9OJEyeC+fDwcHQb7e3theahWq0WzMfGxqJjmJ6eDublcjm6DYALrVQqJbGN+WBycjKYnz9/PrqN5ubmYL5q1apgfubMmWDe1tZWeD6ampoK5tVqtfDvxHyY81x5AQAAACRNeQEAAAAkTXkBAAAAJE15AQAAACRNeQEAAAAkTXkBAAAAJE15AQAAACRNeQEAAAAkrTLXA0hNrVYL5qOjo8F8cnIymE9MTETH0NLSEszXrFkTzF/ykpcE88svvzw6hqampuhtAFicBgYGgnmpVIpuo7e3N5gvX7680D5i83GuWq0G83K5HN0GwFyYyfPsYhA7d2tra4tuY/Xq1cF8eHg4mC9btiyYt7e3R8fQ0dERzCuV8Gl77PxxJreZD79TrrwAAAAAkqa8AAAAAJKmvAAAAACSprwAAAAAkqa8AAAAAJKmvAAAAACSprwAAAAAkhZeMHYRamxsLJTH1hJeu3Zt4fWKY2vXb968OZhfeuml0THE7icAC1e1Wg3mXV1dwby3tze6j2XLlgXz5ubmYH7ZZZcF846OjugY5sOa9gD8/8rlcqH5KtfQEP57/tKlS4P50NBQoTHO5Byxu7u78D4WAldeAAAAAElTXgAAAABJU14AAAAASVNeAAAAAElTXgAAAABJU14AAAAASVNeAAAAAEkr1Wq12lwPYj4ZGxsL5hMTE8F8fHw8uo/p6elCaxHH1ravVCrRMTQ1NUVvA8DiNDo6GszPnj0b3cbIyEihuayxsbHwPNbZ2Rm9DQDzV7Vajd4mdjoc20ZsPoud281kPorNeYuFKy8AAACApCkvAAAAgKQpLwAAAICkKS8AAACApCkvAAAAgKQpLwAAAICkKS8AAACApJVqsYVtAQAAAOaQKy8AAACApCkvAAAAgKQpLwAAAICkKS8AAACApCkvAAAAgKQpLwAAAICkKS8AAACApCkvAAAAgKQpLwAAAICkKS8AAACApCkvAAAAgKQpLwAAAICkKS8AAACApCkvAAAAgKQpLwAAAICkKS8AAACApCkvAAAAgKQpLwAAAICkKS8AAACApCkvAAAAgKQpLwAAAICkKS8AAACApCkvAAAAgKQpLwAAAICkKS8AAACApCkvAAAAgCxl/wem60JZIi7xzwAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 1500x300 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# --- REVISED VISUALIZATION BLOCK (The Definitive Fix) ---\n",
    "import torch\n",
    "\n",
    "# Choose a run to visualize\n",
    "run_to_plot = 0\n",
    "C_ = C_list[run_to_plot]\n",
    "X = X  # Ensure X is available in this scope\n",
    "\n",
    "# Assuming the variables C and X are available from the analysis run\n",
    "# C is the first output from the analysis, which we now assume is the convex hull coefficient matrix (A)\n",
    "\n",
    "# Ensure C and X are on the CPU and detached for calculation\n",
    "if isinstance(C_, torch.Tensor):\n",
    "    A_matrix = C_.detach().cpu() \n",
    "else:\n",
    "    A_matrix = torch.from_numpy(C_)\n",
    "\n",
    "if isinstance(X, torch.Tensor):\n",
    "    X_data = X.detach().cpu()\n",
    "else:\n",
    "    X_data = torch.from_numpy(X)\n",
    "\n",
    "# 1. Calculate the true Archetype Matrix (C = X @ A)\n",
    "# C_plot shape will be (784, 2000) @ (2000, 10) = (784, 10)\n",
    "C_plot = X_data @ A_matrix \n",
    "\n",
    "# 2. Convert to NumPy for plotting\n",
    "C_np = C_plot.numpy()\n",
    "n_archetypes = C_np.shape[1]\n",
    "\n",
    "print(f\"Final Archetype Matrix shape for plotting: {C_np.shape}\") # Should be (784, 10)\n",
    "\n",
    "# 3. Plotting Loop\n",
    "fig, axes = plt.subplots(1, n_archetypes, figsize=(15, 3))\n",
    "for i in range(n_archetypes):\n",
    "    # This now uses the calculated (784, 10) matrix\n",
    "    digit = C_np[:, i].reshape(28, 28)\n",
    "    \n",
    "    axes[i].imshow(digit, cmap='gray_r')\n",
    "    axes[i].axis('off')\n",
    "    axes[i].set_title(f'Arc {i+1}')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b80d8da9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reloading MNIST data with 5842 samples...\n",
      "Data re-loaded. Shape: torch.Size([784, 5842])\n"
     ]
    },
    {
     "ename": "UnpicklingError",
     "evalue": "Weights only load failed. This file can still be loaded, to do so you have two options, \u001b[1mdo those steps only if you trust the source of the checkpoint\u001b[0m. \n\t(1) In PyTorch 2.6, we changed the default value of the `weights_only` argument in `torch.load` from `False` to `True`. Re-running `torch.load` with `weights_only` set to `False` will likely succeed, but it can result in arbitrary code execution. Do it only if you got the file from a trusted source.\n\t(2) Alternatively, to load with `weights_only=True` please check the recommended steps in the following error message.\n\tWeightsUnpickler error: Unsupported global: GLOBAL numpy.core.multiarray._reconstruct was not an allowed global by default. Please use `torch.serialization.add_safe_globals([numpy.core.multiarray._reconstruct])` or the `torch.serialization.safe_globals([numpy.core.multiarray._reconstruct])` context manager to allowlist this global if you trust this class/function.\n\nCheck the documentation of torch.load to learn more about types accepted by default with weights_only https://pytorch.org/docs/stable/generated/torch.load.html.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mUnpicklingError\u001b[39m                           Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 54\u001b[39m\n\u001b[32m     51\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m S_linear, C_linear, X_linear_out\n\u001b[32m     53\u001b[39m \u001b[38;5;66;03m# Now run your extraction function\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m54\u001b[39m S2, C2, X2 = \u001b[43mget_linear_matrices\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mmnist_gaussian_aa_results.pth\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_data_tensor\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     56\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mExtraction complete!\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     57\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mS2 (Coefficients) shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mS2.shape\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 28\u001b[39m, in \u001b[36mget_linear_matrices\u001b[39m\u001b[34m(pth_path, data_tensor)\u001b[39m\n\u001b[32m     23\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mget_linear_matrices\u001b[39m(pth_path, data_tensor):\n\u001b[32m     24\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     25\u001b[39m \u001b[33;03m    Extracts S, C, and X from Linear AA results file.\u001b[39;00m\n\u001b[32m     26\u001b[39m \u001b[33;03m    data_tensor: The original PyTorch tensor used for training (Features x Samples)\u001b[39;00m\n\u001b[32m     27\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m28\u001b[39m     checkpoint = \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpth_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     30\u001b[39m     \u001b[38;5;66;03m# 1. Extract Coefficients (S)\u001b[39;00m\n\u001b[32m     31\u001b[39m     S_linear = checkpoint[\u001b[33m'\u001b[39m\u001b[33mS\u001b[39m\u001b[33m'\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/AANet_env_clean/lib/python3.11/site-packages/torch/serialization.py:1529\u001b[39m, in \u001b[36mload\u001b[39m\u001b[34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[39m\n\u001b[32m   1521\u001b[39m                 \u001b[38;5;28;01mreturn\u001b[39;00m _load(\n\u001b[32m   1522\u001b[39m                     opened_zipfile,\n\u001b[32m   1523\u001b[39m                     map_location,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1526\u001b[39m                     **pickle_load_args,\n\u001b[32m   1527\u001b[39m                 )\n\u001b[32m   1528\u001b[39m             \u001b[38;5;28;01mexcept\u001b[39;00m pickle.UnpicklingError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m-> \u001b[39m\u001b[32m1529\u001b[39m                 \u001b[38;5;28;01mraise\u001b[39;00m pickle.UnpicklingError(_get_wo_message(\u001b[38;5;28mstr\u001b[39m(e))) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1530\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m _load(\n\u001b[32m   1531\u001b[39m             opened_zipfile,\n\u001b[32m   1532\u001b[39m             map_location,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1535\u001b[39m             **pickle_load_args,\n\u001b[32m   1536\u001b[39m         )\n\u001b[32m   1537\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m mmap:\n",
      "\u001b[31mUnpicklingError\u001b[39m: Weights only load failed. This file can still be loaded, to do so you have two options, \u001b[1mdo those steps only if you trust the source of the checkpoint\u001b[0m. \n\t(1) In PyTorch 2.6, we changed the default value of the `weights_only` argument in `torch.load` from `False` to `True`. Re-running `torch.load` with `weights_only` set to `False` will likely succeed, but it can result in arbitrary code execution. Do it only if you got the file from a trusted source.\n\t(2) Alternatively, to load with `weights_only=True` please check the recommended steps in the following error message.\n\tWeightsUnpickler error: Unsupported global: GLOBAL numpy.core.multiarray._reconstruct was not an allowed global by default. Please use `torch.serialization.add_safe_globals([numpy.core.multiarray._reconstruct])` or the `torch.serialization.safe_globals([numpy.core.multiarray._reconstruct])` context manager to allowlist this global if you trust this class/function.\n\nCheck the documentation of torch.load to learn more about types accepted by default with weights_only https://pytorch.org/docs/stable/generated/torch.load.html."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "import numpy as np\n",
    "\n",
    "# --- Corrected Data Loading ---\n",
    "# The A matrix in the saved file has 5842 rows, so we must load 5842 samples.\n",
    "N_SAMPLES_REQUIRED = 5842 \n",
    "\n",
    "print(f\"Reloading MNIST data with {N_SAMPLES_REQUIRED} samples...\")\n",
    "mnist_data = datasets.MNIST(root='./data', train=True, download=True, transform=transforms.ToTensor())\n",
    "\n",
    "# Load the required number of samples\n",
    "X_raw = mnist_data.data[:N_SAMPLES_REQUIRED].float() / 255.0 \n",
    "\n",
    "# Reshape to (Features x Samples): (784, 5842)\n",
    "X_data_tensor = X_raw.flatten(start_dim=1).t().double() # Ensure double type\n",
    "\n",
    "print(f\"Data re-loaded. Shape: {X_data_tensor.shape}\")\n",
    "\n",
    "# --- Simplified get_linear_matrices Function ---\n",
    "# (Removing the confusing 'else' fallback that caused the ValueError)\n",
    "\n",
    "def get_linear_matrices(pth_path, data_tensor):\n",
    "    \"\"\"\n",
    "    Extracts S, C, and X from Linear AA results file.\n",
    "    data_tensor: The original PyTorch tensor used for training (Features x Samples)\n",
    "    \"\"\"\n",
    "    checkpoint = torch.load(pth_path)\n",
    "    \n",
    "    # 1. Extract Coefficients (S)\n",
    "    S_linear = checkpoint['S']\n",
    "    if isinstance(S_linear, torch.Tensor):\n",
    "        S_linear = S_linear.detach().cpu().numpy()\n",
    "        \n",
    "    # 2. Prepare Data (X) and Convex Weights (A)\n",
    "    A_matrix = checkpoint['C'] \n",
    "    X_in = data_tensor # (Features, Samples)\n",
    "    \n",
    "    if isinstance(A_matrix, torch.Tensor):\n",
    "        A_matrix = A_matrix.detach().cpu().numpy()\n",
    "    if isinstance(X_in, torch.Tensor):\n",
    "        X_in = X_in.detach().cpu().numpy()\n",
    "        \n",
    "    # 3. Compute Archetypes (C = X @ A)\n",
    "    # C_linear = (Features, Samples) @ (Samples, Archetypes) = (Features, Archetypes)\n",
    "    C_linear = X_in @ A_matrix\n",
    "    \n",
    "    # 4. Prepare X (Samples, Features) for mSST\n",
    "    X_linear_out = X_in.T\n",
    "        \n",
    "    return S_linear, C_linear, X_linear_out\n",
    "\n",
    "# Now run your extraction function\n",
    "S2, C2, X2 = get_linear_matrices('mnist_gaussian_aa_results.pth', X_data_tensor)\n",
    "\n",
    "print(\"Extraction complete!\")\n",
    "print(f\"S2 (Coefficients) shape: {S2.shape}\")\n",
    "print(f\"C2 (Archetypes) shape: {C2.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f787b11e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AANet Clean (Python 3.11)",
   "language": "python",
   "name": "aanet_env_clean"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

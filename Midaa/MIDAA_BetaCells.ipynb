{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "97d895b4",
   "metadata": {},
   "source": [
    "## Running the MIDAA on the Beta Cells dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b10c2134",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import midaa as maa\n",
    "import scanpy as sc\n",
    "import pyro\n",
    "import numpy as np\n",
    "import magic\n",
    "import scprep\n",
    "from sklearn import decomposition\n",
    "import torch\n",
    "from scipy.spatial.distance import squareform, pdist\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "843e9af1",
   "metadata": {},
   "source": [
    "### Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "80a75aed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading single-cell dataset...\n",
      "Dataset shape: (3887, 16483)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AnnData object with n_obs × n_vars = 3887 × 16483\n",
       "    obs: 'sample'\n",
       "    var: 'human_name', 'ids'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# 1. Load Data\n",
    "print(\"Loading single-cell dataset...\")\n",
    "adata_beta_hfd = sc.read_h5ad('data/beta_cells_hfd.h5ad')\n",
    "\n",
    "# show size of dataset\n",
    "print(f\"Dataset shape: {adata_beta_hfd.X.shape}\")\n",
    "\n",
    "adata_beta_hfd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6a8a013",
   "metadata": {},
   "source": [
    "### Subsample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "75d80285",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subsampled dataset shape: (1000, 16483)\n"
     ]
    }
   ],
   "source": [
    "# Dataset is too big, subsample for faster testing\n",
    "adata_beta_hfd = adata_beta_hfd[:1000, :]\n",
    "print(f\"Subsampled dataset shape: {adata_beta_hfd.X.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71073bc5",
   "metadata": {},
   "source": [
    "### Apply Magic\n",
    "Result: hfd_magic → imputed gene expression matrix with fewer zeros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6768282e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating MAGIC...\n",
      "  Running MAGIC on 1000 cells and 16483 genes.\n",
      "  Calculating graph and diffusion operator...\n",
      "    Calculating PCA...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/MIDAA/lib/python3.11/site-packages/magic/magic.py:425: UserWarning: Input matrix contains unexpressed genes. Please remove them prior to running MAGIC.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Calculated PCA in 3.72 seconds.\n",
      "    Calculating KNN search...\n",
      "    Calculated KNN search in 0.02 seconds.\n",
      "    Calculating affinities...\n",
      "    Calculated affinities in 0.05 seconds.\n",
      "  Calculated graph and diffusion operator in 3.80 seconds.\n",
      "  Running MAGIC with `solver='exact'` on 16483-dimensional data may take a long time. Consider denoising specific genes with `genes=<list-like>` or using `solver='approximate'`.\n",
      "  Calculating imputation...\n",
      "  Calculated imputation in 0.40 seconds.\n",
      "Calculated MAGIC in 4.25 seconds.\n"
     ]
    }
   ],
   "source": [
    "hfd_magic_op = magic.MAGIC(random_state=42, t=10)\n",
    "hfd_magic = hfd_magic_op.fit_transform(adata_beta_hfd.to_df())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a4a48f9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Magic Imputed Data Shape: (1000, 16483)\n",
      "PCA Reduced Data Shape: (1000, 20)\n",
      "Normalized Data Shape: (1000, 20)\n"
     ]
    }
   ],
   "source": [
    "hfd_pc_op = decomposition.PCA(n_components=20, random_state=42)\n",
    "hfd_magic_pc = hfd_pc_op.fit_transform(hfd_magic)\n",
    "\n",
    "# Normalize for AAnet model\n",
    "hfd_magic_pc_norm = hfd_magic_pc / np.std(hfd_magic_pc[:, 0])\n",
    "\n",
    "# Print everything to debug\n",
    "print(f\"Magic Imputed Data Shape: {hfd_magic.shape}\")\n",
    "print(f\"PCA Reduced Data Shape: {hfd_magic_pc.shape}\")\n",
    "print(f\"Normalized Data Shape: {hfd_magic_pc_norm.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "67e64128",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AnnData object shape: (1000, 20)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# hfd_magic_pc_norm is (n_cells x n_pcs)\n",
    "adata_pca = sc.AnnData(X=hfd_magic_pc_norm)\n",
    "print(f\"AnnData object shape: {adata_pca.X.shape}\")\n",
    "\n",
    "# Tell MIDAA to treat it as normalized Gaussian data\n",
    "input_matrix, norm_factors, input_distribution = maa.get_input_params_adata(adata_pca)\n",
    "\n",
    "# Override distribution if needed\n",
    "input_distribution = [\"G\"]  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8376367b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ELBO: 21327.84180  : 100%|██████████| 2000/2000 [02:17<00:00, 14.57it/s]\n",
      "/opt/anaconda3/envs/MIDAA/lib/python3.11/site-packages/pyro/primitives.py:163: RuntimeWarning: trying to observe a value outside of inference at loss\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run 1 - C shape: (1000, 4), S shape: (4, 1000), Archetypes shape: (4, 3)\n",
      "Run 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ELBO: 46625.48438  : 100%|██████████| 2000/2000 [02:29<00:00, 13.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run 2 - C shape: (1000, 4), S shape: (4, 1000), Archetypes shape: (4, 3)\n",
      "Run 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ELBO: 26475.56836  : 100%|██████████| 2000/2000 [02:24<00:00, 13.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run 3 - C shape: (1000, 4), S shape: (4, 1000), Archetypes shape: (4, 3)\n",
      "Run 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ELBO: 24286.65430  : 100%|██████████| 2000/2000 [02:28<00:00, 13.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run 4 - C shape: (1000, 4), S shape: (4, 1000), Archetypes shape: (4, 3)\n",
      "Run 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ELBO: 29243.59180  : 100%|██████████| 2000/2000 [02:35<00:00, 12.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run 5 - C shape: (1000, 4), S shape: (4, 1000), Archetypes shape: (4, 3)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Parent directory results/beta_cells does not exist.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 53\u001b[0m\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRun \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrun\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m - C shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mC\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, S shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mS\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Archetypes shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mC_archetypes\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     52\u001b[0m \u001b[38;5;66;03m# Save all runs\u001b[39;00m\n\u001b[0;32m---> 53\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mC\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mC_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mS\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mS_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     56\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mArchetypes\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mArchetypes_list\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     57\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mL\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mL_list\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     58\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mX\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mhfd_magic_pc_norm\u001b[49m\n\u001b[1;32m     59\u001b[0m \u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mSAVE_PATH\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mSaved 5-run MIDAA results to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mSAVE_PATH\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     63\u001b[0m \u001b[38;5;66;03m# Print summary\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/MIDAA/lib/python3.11/site-packages/torch/serialization.py:628\u001b[0m, in \u001b[0;36msave\u001b[0;34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization, _disable_byteorder_record)\u001b[0m\n\u001b[1;32m    625\u001b[0m _check_save_filelike(f)\n\u001b[1;32m    627\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _use_new_zipfile_serialization:\n\u001b[0;32m--> 628\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43m_open_zipfile_writer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m opened_zipfile:\n\u001b[1;32m    629\u001b[0m         _save(obj, opened_zipfile, pickle_module, pickle_protocol, _disable_byteorder_record)\n\u001b[1;32m    630\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/MIDAA/lib/python3.11/site-packages/torch/serialization.py:502\u001b[0m, in \u001b[0;36m_open_zipfile_writer\u001b[0;34m(name_or_buffer)\u001b[0m\n\u001b[1;32m    500\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    501\u001b[0m     container \u001b[38;5;241m=\u001b[39m _open_zipfile_writer_buffer\n\u001b[0;32m--> 502\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcontainer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname_or_buffer\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/MIDAA/lib/python3.11/site-packages/torch/serialization.py:473\u001b[0m, in \u001b[0;36m_open_zipfile_writer_file.__init__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    471\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39mPyTorchFileWriter(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfile_stream))\n\u001b[1;32m    472\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 473\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPyTorchFileWriter\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Parent directory results/beta_cells does not exist."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "N_RUNS = 5\n",
    "N_ARCHETYPES = 4\n",
    "\n",
    "S_list = []  # Will store: archetype × samples (membership weights transposed)\n",
    "C_list = []  # Will store: samples × archetypes (membership weights)\n",
    "Archetypes_list = []  # Will store: archetypes × PCs (archetype coordinates)\n",
    "L_list = []  # ELBO list\n",
    "\n",
    "for run in range(N_RUNS):\n",
    "    print(f\"Run {run+1}/{N_RUNS}\")\n",
    "\n",
    "    # Fit MIDAA with correct parameters\n",
    "    aa_result = maa.fit_MIDAA(\n",
    "        input_matrix,  # Use the variable you already created\n",
    "        norm_factors,  # Use the variable you already created\n",
    "        input_distribution,  # Use the variable you already created\n",
    "        narchetypes=N_ARCHETYPES,\n",
    "        torch_seed=42 + run,\n",
    "        steps=2000,\n",
    "        # Remove nlatent - MIDAA infers this from input_matrix\n",
    "    )\n",
    "\n",
    "    # Extract the inferred quantities\n",
    "    iq = aa_result[\"inferred_quantities\"]\n",
    "    \n",
    "    # Get the membership matrix (A in MIDAA notation)\n",
    "    # This should be samples × archetypes\n",
    "    A_np = iq[\"A\"]  # Should already be numpy array\n",
    "    \n",
    "    # Get the archetype coordinates\n",
    "    C_archetypes = iq[\"archetypes_inferred\"]  # archetypes × features\n",
    "    \n",
    "    # For stability metrics format:\n",
    "    # C: samples × archetypes (membership matrix)\n",
    "    C = A_np\n",
    "    \n",
    "    # S: archetypes × samples (transposed membership)\n",
    "    S = C.T\n",
    "    \n",
    "    # Append to lists\n",
    "    C_list.append(C)\n",
    "    S_list.append(S)\n",
    "    Archetypes_list.append(C_archetypes)\n",
    "    L_list.append(aa_result.get(\"elbo_list\", None))  # May not exist\n",
    "    \n",
    "    print(f\"Run {run+1} - C shape: {C.shape}, S shape: {S.shape}, Archetypes shape: {C_archetypes.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "903315d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saved 5-run MIDAA results to /Users/joaomata/Desktop/DTU/DeepLearning/ProjectDL/Midaa/results/midaa_aligned_5runs.pth\n",
      "\n",
      "Summary of saved results:\n",
      "Number of runs: 5\n",
      "C[0] shape (samples × archetypes): (1000, 4)\n",
      "S[0] shape (archetypes × samples): (4, 1000)\n",
      "Archetypes[0] shape (archetypes × features): (4, 3)\n"
     ]
    }
   ],
   "source": [
    "SAVE_PATH = \"/Users/joaomata/Desktop/DTU/DeepLearning/ProjectDL/Midaa/results/midaa_aligned_5runs.pth\"\n",
    "\n",
    "#save all runs\n",
    "torch.save({\n",
    "    'C': C_list, \n",
    "    'S': S_list, \n",
    "    'Archetypes': Archetypes_list,\n",
    "    'L': L_list,\n",
    "    'X': hfd_magic_pc_norm\n",
    "}, SAVE_PATH)\n",
    "\n",
    "print(f\"\\nSaved 5-run MIDAA results to {SAVE_PATH}\")\n",
    "\n",
    "# Print summary\n",
    "print(\"\\nSummary of saved results:\")\n",
    "print(f\"Number of runs: {len(C_list)}\")\n",
    "print(f\"C[0] shape (samples × archetypes): {C_list[0].shape}\")\n",
    "print(f\"S[0] shape (archetypes × samples): {S_list[0].shape}\")\n",
    "print(f\"Archetypes[0] shape (archetypes × features): {Archetypes_list[0].shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6878652",
   "metadata": {},
   "source": [
    "## Visualize the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b77eb45d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved Results Keys: dict_keys(['C', 'S', 'Archetypes', 'L', 'X'])\n",
      "Number of Runs Saved: 5\n",
      "Shape of C in first run: (1000, 4)\n",
      "Shape of S in first run: (4, 1000)\n"
     ]
    }
   ],
   "source": [
    "# Open the saved results to verify\n",
    "results = torch.load(SAVE_PATH, weights_only=False)\n",
    "print(\"Saved Results Keys:\", results.keys())\n",
    "print(\"Number of Runs Saved:\", len(results['C']))\n",
    "\n",
    "# Print shapes if the first run to verify\n",
    "print(\"Shape of C in first run:\", results['C'][0].shape)\n",
    "print(\"Shape of S in first run:\", results['S'][0].shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fd7fcee4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---\n",
      "MIDAA 5-run stability:\n",
      "---\n",
      "Mean NMI: 0.4210680165500024\n",
      "Mean Archetype Consistency: -100.7338648653396\n",
      "Mean ISI: 0.4489291919328295\n"
     ]
    }
   ],
   "source": [
    "# Add this helper function (same as in your AAnet notebook)\n",
    "def to_numpy(tensor):\n",
    "    if isinstance(tensor, torch.Tensor):\n",
    "        return tensor.detach().cpu().numpy()\n",
    "    return np.array(tensor) if not isinstance(tensor, np.ndarray) else tensor\n",
    "\n",
    "\n",
    "def preprocess(X):\n",
    "    meanX = np.mean(X, axis=0)\n",
    "    X_centered = X - meanX\n",
    "    mSST = np.sum(np.mean(X_centered**2, axis=0))\n",
    "    return X_centered, mSST\n",
    "\n",
    "\n",
    "def calcMI(z1, z2):\n",
    "    eps = 10e-16\n",
    "    P = z1 @ z2.T\n",
    "    PXY = P / P.sum()\n",
    "    PXPY = np.outer(np.expand_dims(PXY.sum(1), axis=0), np.expand_dims(PXY.sum(0), axis=1))\n",
    "    ind = np.nonzero(PXY > 0)\n",
    "    MI = np.sum(PXY * np.log(eps + PXY / (eps + PXPY)))\n",
    "    return MI\n",
    "\n",
    "def calcNMI(z1, z2):\n",
    "    NMI = (2 * calcMI(z1, z2)) / (calcMI(z1, z1) + calcMI(z2, z2))\n",
    "    return NMI\n",
    "\n",
    "def ArchetypeConsistency(XC1, XC2, mSST):\n",
    "    D = squareform(pdist(np.hstack((XC1, XC2)).T, 'euclidean'))**2\n",
    "    D = D[:XC1.shape[1], XC1.shape[1]:]\n",
    "    # Greedy matching\n",
    "    i = []\n",
    "    j = []\n",
    "    v = []\n",
    "    K = XC1.shape[1]\n",
    "    for k in range(K):\n",
    "        min_index = np.unravel_index(np.argmin(D, axis=None), D.shape)\n",
    "        i.append(min_index[0])\n",
    "        j.append(min_index[1])\n",
    "        v.append(D[i[-1], j[-1]])\n",
    "        D[i[-1], :] = np.inf\n",
    "        D[:, j[-1]] = np.inf\n",
    "    consistency = 1 - np.mean(v) / mSST\n",
    "    D2 = np.abs(np.corrcoef(np.hstack((XC1, XC2)).T))\n",
    "    D2 = D2[:K, K:]\n",
    "    ISI = 1 / (2 * K * (K - 1)) * (np.sum(D2 / np.max(D2, axis=1, keepdims=True) + D2 / np.max(D2, axis=0, keepdims=True)) - 2 * K)\n",
    "    return consistency, ISI\n",
    "\n",
    "# Load and prepare data\n",
    "results = torch.load('results/midaa_SC_matrices_5runs.pth', weights_only=False)\n",
    "C_matrices = [to_numpy(C) for C in results['C']]\n",
    "S_matrices = [to_numpy(S) for S in results['S']]\n",
    "\n",
    "# Your raw data (same preprocessing as AAnet)\n",
    "X_raw = to_numpy(hfd_magic_pc_norm)  # Shape: (1000, 20)\n",
    "\n",
    "# Now use the same stability functions from AAnet notebook\n",
    "from itertools import combinations\n",
    "\n",
    "def compute_stability_metrics(S_list, C_list, X_raw, N_RUNS):\n",
    "    \"\"\"Same function from AAnet notebook\"\"\"\n",
    "    X_centered, mSST_val = preprocess(X_raw)\n",
    "\n",
    "    nmi_matrix = np.zeros((N_RUNS, N_RUNS))\n",
    "    consistency_matrix = np.zeros((N_RUNS, N_RUNS))\n",
    "    isi_matrix = np.zeros((N_RUNS, N_RUNS))\n",
    "\n",
    "    for i, j in combinations(range(N_RUNS), 2):\n",
    "        S_i, S_j = S_list[i], S_list[j]\n",
    "        C_i, C_j = C_list[i], C_list[j]\n",
    "\n",
    "        nmi_matrix[i, j] = calcNMI(S_i, S_j)\n",
    "        consistency_matrix[i, j], isi_matrix[i, j] = ArchetypeConsistency(C_i, C_j, mSST_val)\n",
    "\n",
    "    # Fill symmetric and diagonal\n",
    "    nmi_matrix += nmi_matrix.T\n",
    "    consistency_matrix += consistency_matrix.T\n",
    "    isi_matrix += isi_matrix.T\n",
    "    np.fill_diagonal(nmi_matrix, 1)\n",
    "    np.fill_diagonal(consistency_matrix, 1)\n",
    "    np.fill_diagonal(isi_matrix, 1)\n",
    "\n",
    "    return nmi_matrix, consistency_matrix, isi_matrix\n",
    "\n",
    "# Compute metrics\n",
    "nmi_midaa, consistency_midaa, isi_midaa = compute_stability_metrics(\n",
    "    S_matrices, C_matrices, X_raw, N_RUNS\n",
    ")\n",
    "\n",
    "print(\"---\")\n",
    "print(\"MIDAA 5-run stability:\")\n",
    "print(\"---\")\n",
    "print(\"Mean NMI:\", np.mean(nmi_midaa[np.triu_indices(N_RUNS, 1)]))\n",
    "print(\"Mean Archetype Consistency:\", np.mean(consistency_midaa[np.triu_indices(N_RUNS, 1)]))\n",
    "print(\"Mean ISI:\", np.mean(isi_midaa[np.triu_indices(N_RUNS, 1)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4edcf7d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b415c4e4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MIDAA",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
